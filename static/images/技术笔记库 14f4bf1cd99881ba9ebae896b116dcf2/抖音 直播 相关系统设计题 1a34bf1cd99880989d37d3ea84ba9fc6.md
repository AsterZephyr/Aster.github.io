# 抖音/直播  相关系统设计题

Created: 2025年2月23日 16:45
Status: 完成

## 场景设计：抖音点赞功能如何实现？如何实现点赞数实时展示？春节或者奥运会期间，全红婵、大V发的一个热门视频或者投了抖+的视频，很多人都会点赞，基于这个场景，你这个点赞功能或者接口怎么做？要求承受高并发；在高并发的前提下，我们点赞量的一个统计尽可能实时，要求前一个点赞在0.1s之后，可以被别的用户看到，要求比较低的一个延时性，
基于这两个要求，请说说你的方案或者你要从哪些方面进行考虑。
追问：
（1）你如何做负载均衡？
（2）如何保证负载均衡的每一个用户耗时不会有太大的差别？
（3）多机房部署，你的缓存如何保证数据一致性呢，保证每一个缓存节点都可以拿到全局的数据？
（4)分布式服务如何选取主节点？你可以选择你熟悉的所有的分布式集群；

### 一、抖音高并发点赞功能设计方案

### 1. **核心架构设计**

- **分层缓存策略**：
    - **本地缓存**：采用Caffeine或Guava缓存热点视频的点赞数，TTL设置为100ms，通过最小堆算法动态识别热点视频[6](https://www.notion.so/@ref)。
    - **Redis集群**：使用Redis Hash存储用户点赞记录（Key: `video:{videoId}:likes`），Redis原子计数器（`INCR/DECR`）实时更新点赞数，通过Lua脚本保证原子性[34](https://www.notion.so/@ref)。
    - **多级缓存一致性**：通过Redis Pub/Sub实现本地缓存失效通知，确保跨节点数据一致性[6](https://www.notion.so/@ref)。
- **异步削峰处理**：
    - 用户点赞操作写入Kafka队列，由消费者批量聚合写入数据库，降低DB压力[316](https://www.notion.so/@ref)。
    - 采用**合并写入**策略（如10秒内聚合点赞数批量更新），减少IO次数[6](https://www.notion.so/@ref)。

### 2. **实时性保障**

- **近实时更新**：
    - 用户点赞后，优先更新Redis计数器，前端通过WebSocket或SSE（Server-Sent Events）主动推送最新点赞数，确保0.1s内可见[46](https://www.notion.so/@ref)。
    - 使用**布隆过滤器**快速判断用户是否已点赞，避免重复查询[3](https://www.notion.so/@ref)。
- **数据持久化**：
    - 定时任务（如5分钟）将Redis数据异步写入MySQL，采用**最终一致性**模型[34](https://www.notion.so/@ref)。
    - 数据库设计：分库分表（按视频ID Hash分片），TiDB或ShardingSphere实现水平扩展[166](https://www.notion.so/@ref)。

### 3. **高并发应对**

- **服务分层**：
    - **API网关层**：统一鉴权、限流（令牌桶算法），拦截恶意请求[616](https://www.notion.so/@ref)。
    - **无状态服务**：点赞服务实例无状态化，通过Consul或Nacos实现服务注册与发现[24](https://www.notion.so/@ref)。
- **热点处理**：
    - **动态分片**：对热门视频（如全红婵视频）单独分片，采用Redis Cluster分片扩容[6](https://www.notion.so/@ref)。
    - **本地热点缓存**：识别热点视频后，在服务节点内存中缓存点赞数，TTL 100ms[6](https://www.notion.so/@ref)。

---

### 二、追问问题深度解析

### （1）负载均衡设计

- **全局负载均衡**：
    - DNS轮询 + 多机房GSLB（全局负载均衡），根据用户地理位置分配最近机房[624](https://www.notion.so/@ref)。
    - 机房内采用Nginx加权轮询（根据服务器CPU/内存负载动态调整权重）[16](https://www.notion.so/@ref)。
- **服务层负载均衡**：
    - 使用**一致性哈希算法**，将同一视频的请求路由到固定服务节点，减少缓存穿透[316](https://www.notion.so/@ref)。
    - 结合**最小响应时间策略**，动态选择后端节点[8](https://www.notion.so/@ref)。

### （2）耗时均衡保障

- **动态权重调整**：
    - 监控各节点耗时（Prometheus + Grafana），超过阈值（如P99>200ms）自动降低权重[1614](https://www.notion.so/@ref)。
    - 采用**自适应限流**：Sentinel或Hystrix根据节点压力动态限流[16](https://www.notion.so/@ref)。
- **请求分桶**：
    - 对高耗时操作（如DB写入）拆分为独立线程池，避免阻塞主线程[8](https://www.notion.so/@ref)。

### （3）多机房缓存一致性

- **缓存同步机制**：
    - **跨机房Redis集群**：采用Redis主从复制 + 异步哨兵模式，容忍秒级延迟[6](https://www.notion.so/@ref)。
    - **双写+版本号**：写入时双写两地Redis，通过版本号冲突解决（类似CRDT）[8](https://www.notion.so/@ref)。
- **最终一致性方案**：
    - 通过Binlog监听MySQL变更，由消息队列同步至其他机房[6](https://www.notion.so/@ref)。
    - 使用**Tair**或自研KV数据库（如B站TaiShan）实现跨机房数据同步[6](https://www.notion.so/@ref)。

### （4）分布式服务选主策略

- **主流方案对比**：
    - **ZooKeeper**：基于ZAB协议，适合强一致性场景（如配置中心）[24](https://www.notion.so/@ref)。
    - **Etcd**：Raft协议实现，Kubernetes生态首选（如服务发现）[24](https://www.notion.so/@ref)。
    - **Redis Sentinel**：简单选主，适合无状态服务[6](https://www.notion.so/@ref)。
- **抖音推荐方案**：
    - **Region-based选举**：每个机房内独立选主，通过全局协调服务（如Chubby）同步状态[6](https://www.notion.so/@ref)。
    - **Raft协议**：TiDB等分布式数据库内置Raft，天然支持多机房数据一致性[6](https://www.notion.so/@ref)。

---

### 三、容灾与扩展性设计

1. **降级策略**：
    - 缓存宕机时，降级为本地内存计数，限制写入速率[6](https://www.notion.so/@ref)。
    - DB宕机时，依赖Redis持久化（AOF）临时提供服务[6](https://www.notion.so/@ref)。
2. **自动化扩缩容**：
    - 基于Kubernetes HPA，根据CPU/内存指标自动扩展服务节点[16](https://www.notion.so/@ref)。
    - Redis Cluster支持弹性扩缩容，通过resharding自动迁移数据[3](https://www.notion.so/@ref)。
3. **压测与监控**：
    - 全链路压测（如春节前模拟流量峰值），使用Jmeter或TSung[1417](https://www.notion.so/@ref)。
    - 全维度监控：API耗时、Redis命中率、DB连接池状态[614](https://www.notion.so/@ref)。

---